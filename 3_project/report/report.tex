\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Data Mining: Learning from Large Data Sets - Fall Semester 2015}
\author{mmarti@student.ethz.ch\\ trubeli@student.ethz.ch}
\date{\today}

\begin{document}
\maketitle

\section*{Extracting Representative Elements} 
For this project we used a parallel version of k-means on our dataset. In a first step we designed an algorithm that could run on an arbitrary number of map processes and that could be combined in one reduce process.
\vspace{10pt}
\\Firstly, we choose $k$ center randomly. Each map process gets a seed for the random number generator in order to make sure that the same centers are picked in each map process.
The map process reads on the standard input and assigns each data point to one of the $k_{i}$ centers. The reduce process sums all the data points assigned to each center $k_{i}$ and compute the mean of this subset. This gives a new center which is used for these data points.


 

\end{document} 
