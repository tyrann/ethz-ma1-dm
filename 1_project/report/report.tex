\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\algnewcommand{\Initialize}[1]{%
	\State \textbf{Initialize:}
	\Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Data Mining: Learning from Large Data Sets - Fall Semester 2015}
\author{mmarti@student.ethz.ch\\ trubeli@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Approximate near-duplicate search using Locality Sensitive Hashing} 
In this project we used linear hashing to approximate the similarity between videos, represented by a list of shingles. The first step in our solution was to produce a signature matrix. This matrix is obtained by using a min hash algorithm on each list of shingles. For every $i^{th}$ shingle in a video we pick two random numbers $a_{i}$ and $b_{i}$ which are coprime. 
The hash function is applied by computing:
\begin{center}
	$a_{i}*s_{i} + b_{i} \mod{n}$
\end{center}
Where $s_{i}$ is the $i^{th}$ shingle in a video and $n$ is the number of shingles.
The procedure for computing the signature matrix is described as follow:
\vspace{8pt}
\begin{algorithm}
	\caption{Min Hash Algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{MinHash}{$N,K$}\Comment{K hash fonction applied on each of the N shingles}
		\Initialize{\strut$w_l \gets \infty$, $l=1,\ldots,k$}	
		\For{$i = 1 $ \textbf{to} $n$}
			\For{$j = 1 $ \textbf{to} $k$}
			\If{$h_{j}(n_{i}) < w_{j}$}
				\State{$w_{j} \gets h_{j}(n_{i})$}
			\EndIf
			\EndFor
		\EndFor
	
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\\This signature matrix is then split into $b$ bands of $r$ rows. In order to decrease the number false positive and false negative, we decided to use a combined r-way $AND$ and b-way $OR$ of hash function on the split matrix. While the original set of hash function was:
\vspace{8pt}
\begin{center}
	$(d_{1},d_{2},p_{1},p_{2})$-sensitive
\end{center}
\vspace{8pt}
for some $p_{1}$, $p_{2}$. The r-way $AND$ turns $F$ into a new family $F'$ which consists of vectors of hash functions in $F$. We therefore obtain a new family of hash functions s.t. each function in $F'$ is:
\vspace{8pt}
\begin{center}
	$(d_{1},d_{2},p_{1}^r,p_{2}^r)$-sensitive
\end{center}
\vspace{8pt}
Then, applying a b-way $OR$, we just require that at least one band has to be hashed to the same bucket in order for two videos to be considered similar. Therefore turning our family of hash functions into:
\vspace{8pt}
\begin{center}
	$(d_{1},d_{2},1-(1-p_{1}^r)^b,1-(1-p_{2}^r)^b)$-sensitive
\end{center}
\vspace{8pt}
The values of $b$ and $r$ have been tuned accordingly in order to find a sufficient ratio between false positive and false negative. Using values such as 64 bands and 16 rows per band, we managed to obtain a score of 0.97.

\section*{Scripts to simplify local testing}
Seqeuenteially computing results for the test data we were given took quite a lot of time, even on a fast
computer. This was due to the fact that the solution ran on only one core instead of all available cores.
To simplify and speed up the local testing process we implemented a set of small scripts, written in Ruby,
which not only make sure that our mappers and reducers are ran in parallel, but also automatically check
the solution and keep track of the solution with the best score.
\vspace{8pt}

The scripts support two modes of operation, one for manual runs of the current mapper and reducer and one
for continuous testing of different mappers and reducers. The manual approach takes the current mapper and
reducer and runs multiple instances of each in parallel to ensure maximum performance. The intermediate 
sorting is done directly in Ruby. A more efficient approach is the continuous one. Here, we run a separate
script which will periodically check a folder for new entries. Entries are added by executing another script,
which will create a new folder in the \textbf{queue} and copy the current mapper and reducer into it. The observing script will then find this new folder and run the map-reduce algorithm on it. Once it is done, it checks the outcome of the process using the provided check script and moves the folder from the queue directory to a done directory, where it creates files with all intermediate outputs. Additionally, if the F1 value of the tested instance is higher than the best recorded F1 value, the instance is also copied into a special folder named \textbf{best} in the done folder and the maximum F1 value is updated.
\vspace{8pt}

This approach allows us to push solutions to be tested faster than they are executed. We can think
about parameters that make sense, push them onto the queue and continue working while our results are
being computed.
\end{document} 
